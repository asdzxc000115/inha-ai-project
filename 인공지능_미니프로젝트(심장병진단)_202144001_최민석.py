# -*- coding: utf-8 -*-
"""ì¸ê³µì§€ëŠ¥ ë¯¸ë‹ˆí”„ë¡œì íŠ¸(ì‹¬ì¥ë³‘ì§„ë‹¨)_202144001_ìµœë¯¼ì„

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N7-J8AHCJgNiuF9z6TOlV3FtV3pFScr3
"""

# ì‹¬ì¥ë³‘ ì§„ë‹¨ ë¶„ë¥˜ í”„ë¡œì íŠ¸ (202144001_ìµœë¯¼ì„)
# Heart Disease Classification Project

import numpy as np
import pandas as pd
from tensorflow import keras
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# Colabì—ì„œ í•œê¸€ í°íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
# !apt-get install -y fonts-nanum
# !sudo fc-cache -fv
# !rm ~/.cache/matplotlib -rf
# plt.rc('font', family='NanumBarunGothic')

print("="*60)
print("        ì‹¬ì¥ë³‘ ì§„ë‹¨ ë¶„ë¥˜ AI ëª¨ë¸ ê°œë°œ")
print("     (ìˆ˜ì—… ë‚´ìš© ê¸°ë°˜ - Keras & Dense Layer)")
print("="*60)

# =====================================
# 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸
# =====================================
print("\n1. ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰")
print("-"*40)

# UCI Heart Disease Dataset ë¡œë“œ
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
]

df = pd.read_csv(url, names=column_names)

# ê¸°ë³¸ ì •ë³´ ì¶œë ¥
print(f"ë°ì´í„° í˜•íƒœ: {df.shape}")
print(f"íŠ¹ì„±(feature) ê°œìˆ˜: {df.shape[1]-1}")
print(f"ìƒ˜í”Œ ê°œìˆ˜: {df.shape[0]}")

print(f"\në°ì´í„° ì²˜ìŒ 5ê°œ í–‰:")
print(df.head())

print(f"\nê° íŠ¹ì„±ë³„ ì •ë³´:")
print(df.info())

# =====================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ (ìˆ˜ì—… ë°©ì‹)
# =====================================
print("\n2. ë°ì´í„° ì „ì²˜ë¦¬")
print("-"*40)

# ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬ ('?'ë¥¼ NaNìœ¼ë¡œ ë³€í™˜)
df_clean = df.replace('?', np.nan)

# ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜
for col in df_clean.columns:
    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

print("ê²°ì¸¡ì¹˜ í™•ì¸:")
print(df_clean.isnull().sum())

# ê²°ì¸¡ì¹˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ê¸° (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ë°©ë²•)
df_clean = df_clean.fillna(df_clean.mean())

# íƒ€ê²Ÿ ë³€ìˆ˜ ì´ì§„í™” (0: ì •ìƒ, 1: ì‹¬ì¥ë³‘)
df_clean['target'] = (df_clean['target'] > 0).astype(int)

print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í˜•íƒœ: {df_clean.shape}")
print(f"ì‹¬ì¥ë³‘ í™˜ì ìˆ˜: {df_clean['target'].sum()}")
print(f"ì •ìƒì¸ ìˆ˜: {len(df_clean) - df_clean['target'].sum()}")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print(f"\ní´ë˜ìŠ¤ ë¶„í¬:")
print(df_clean['target'].value_counts())

# =====================================
# 3. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™” (ìˆ˜ì—… ë°©ì‹)
# =====================================
print("\n3. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”")
print("-"*40)

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = df_clean.drop('target', axis=1)
y = df_clean['target']

print(f"ì…ë ¥ íŠ¹ì„±: {list(X.columns)}")

# í›ˆë ¨:í…ŒìŠ¤íŠ¸ = 8:2 ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (í›ˆë ¨ ë°ì´í„°ì˜ 20%)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

# MinMaxScalerë¥¼ ì‚¬ìš©í•œ 0~1 ì •ê·œí™” (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ë°©ë²•)
mms = MinMaxScaler()
mms.fit(X_train)

X_train_scaled = mms.transform(X_train)
X_val_scaled = mms.transform(X_val)
X_test_scaled = mms.transform(X_test)

# DataFrameìœ¼ë¡œ ë³€í™˜
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

print("ë°ì´í„° ì •ê·œí™” ì™„ë£Œ (0~1 ë²”ìœ„)")
print(f"ì •ê·œí™” í›„ í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ:")
print(X_train_scaled.head())

# =====================================
# 4. ëª¨ë¸ êµ¬ì„± (ìˆ˜ì—… ë°©ì‹ - Dense Layer)
# =====================================
print("\n4. ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì„±")
print("-"*40)

# Sequential ëª¨ë¸ ìƒì„± (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ë°©ì‹)
model = Sequential()

# Dense Layer ì¶”ê°€ (Fully Connected Layer)
model.add(Dense(128, activation='relu', input_shape=(13,)))  # ì…ë ¥ì¸µ
model.add(Dropout(0.3))                                     # ë“œë¡­ì•„ì›ƒ
model.add(Dense(64, activation='relu'))                     # ì€ë‹‰ì¸µ 1
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))                     # ì€ë‹‰ì¸µ 2
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))                     # ì€ë‹‰ì¸µ 3
model.add(Dense(1, activation='sigmoid'))                   # ì¶œë ¥ì¸µ (ì´ì§„ë¶„ë¥˜)

# ëª¨ë¸ ì»´íŒŒì¼ (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ì„¤ì •)
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # ì´ì§„ ë¶„ë¥˜ìš©
    metrics=['accuracy']         # ìˆ˜ì—…ì—ì„œ ì‚¬ìš©í•œ ì§€í‘œ
)

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
print("ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¡°:")
model.summary()

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° (ìˆ˜ì—… ë‚´ìš© ì°¸ê³ )
total_params = model.count_params()
print(f"\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")

# =====================================
# 5. ì½œë°± ì„¤ì • (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ë°©ë²•)
# =====================================
print("\n5. í•™ìŠµ ì„¤ì •")
print("-"*40)

# Early Stopping (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ë°©ë²•)
early_stopping = EarlyStopping(
    monitor='val_accuracy',      # ê²€ì¦ ì •í™•ë„ ëª¨ë‹ˆí„°ë§
    patience=15,                 # 15 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    restore_best_weights=True,   # ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë³µì›
    verbose=1
)

# Model Checkpoint (ëª¨ë¸ ì €ì¥)
model_checkpoint = ModelCheckpoint(
    'best_heart_disease_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

print("Early Stopping ì„¤ì • ì™„ë£Œ")
print("Model Checkpoint ì„¤ì • ì™„ë£Œ")

# =====================================
# 6. ëª¨ë¸ í•™ìŠµ
# =====================================
print("\n6. ëª¨ë¸ í•™ìŠµ")
print("-"*40)

# ëª¨ë¸ í›ˆë ¨
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=100,                  # ì¶©ë¶„í•œ ì—í¬í¬
    batch_size=32,              # ìˆ˜ì—…ì—ì„œ ì‚¬ìš©í•œ ë°°ì¹˜ í¬ê¸°
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)

# ìµœê³  ëª¨ë¸ ë¡œë“œ
model.load_weights('best_heart_disease_model.h5')
print("\nìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# =====================================
# 7. ì„±ëŠ¥ í‰ê°€ (ìˆ˜ì—…ì—ì„œ ë°°ìš´ ì§€í‘œ)
# =====================================
print("\n7. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
print("-"*40)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)

print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
print(f"í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}")

# í›ˆë ¨ ê³¼ì • ë¶„ì„
best_val_accuracy = max(history.history['val_accuracy'])
best_epoch = history.history['val_accuracy'].index(best_val_accuracy) + 1

print(f"\ní•™ìŠµ ê³¼ì • ë¶„ì„:")
print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f} (ì—í¬í¬ {best_epoch})")
print(f"ì´ í•™ìŠµ ì—í¬í¬: {len(history.history['loss'])}")

# ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸
y_pred_proba = model.predict(X_test_scaled)
y_pred = (y_pred_proba > 0.5).astype(int)

print(f"\nì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
for i in range(10):
    actual = y_test.iloc[i]
    predicted = y_pred[i][0]
    confidence = y_pred_proba[i][0]
    status = "âœ“" if actual == predicted else "âœ—"
    print(f"{status} ì‹¤ì œ: {actual}, ì˜ˆì¸¡: {predicted}, í™•ì‹ ë„: {confidence:.3f}")

# =====================================
# 8. ì‹œê°í™” (í•™ìŠµ ê³¼ì •)
# =====================================
print("\n8. í•™ìŠµ ê³¼ì • ì‹œê°í™”")
print("-"*40)

# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
plt.figure(figsize=(15, 5))

# ì •í™•ë„ ê³¡ì„ 
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# ì†ì‹¤ ê³¡ì„ 
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# íŠ¹ì„± ì¤‘ìš”ë„ (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜)
plt.subplot(1, 3, 3)
weights = model.layers[0].get_weights()[0]  # ì²« ë²ˆì§¸ Dense layer ê°€ì¤‘ì¹˜
feature_importance = np.mean(np.abs(weights), axis=1)
feature_names = X.columns

# ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ í‘œì‹œ
top_indices = np.argsort(feature_importance)[-10:]
plt.barh(range(len(top_indices)), feature_importance[top_indices])
plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])
plt.title('Feature Importance (Top 10)')
plt.xlabel('Importance')

plt.tight_layout()
plt.show()

# =====================================
# 9. í˜¼ë™í–‰ë ¬ ë° ìƒì„¸ ë¶„ì„
# =====================================
from sklearn.metrics import confusion_matrix, classification_report

print("\n9. ìƒì„¸ ì„±ëŠ¥ ë¶„ì„")
print("-"*40)

# í˜¼ë™í–‰ë ¬
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Normal', 'Heart Disease'], yticklabels=['Normal', 'Heart Disease'])
plt.title('Confusion Matrix')
plt.ylabel('True')
plt.xlabel('Predicted')
plt.show()

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Heart Disease']))

# =====================================
# 10. ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸
# =====================================
print("\n" + "="*60)
print("                  í”„ë¡œì íŠ¸ ê²°ë¡ ")
print("="*60)

print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")
print(f"âœ… ì‚¬ìš©ëœ ê¸°ë²•:")
print(f"   - Dense Layer (Fully Connected Layer)")
print(f"   - MinMaxScalerë¥¼ ì´ìš©í•œ ì •ê·œí™”")
print(f"   - Early Stoppingìœ¼ë¡œ ê³¼ì í•© ë°©ì§€")
print(f"   - Model Checkpointë¡œ ìµœê³  ëª¨ë¸ ì €ì¥")
print(f"   - Dropoutìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")

print(f"\nğŸ¥ ì˜ë£Œì§„ì„ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:")
print(f"   - ì‹¬ì¥ë³‘ ì§„ë‹¨ ë³´ì¡° ë„êµ¬ë¡œ í™œìš© ê°€ëŠ¥")
print(f"   - {test_accuracy*100:.1f}% ì •í™•ë„ë¡œ ì‹ ë¢°í•  ë§Œí•œ ì„±ëŠ¥")
print(f"   - ì´ {total_params:,}ê°œ íŒŒë¼ë¯¸í„°ë¡œ íš¨ìœ¨ì ì¸ ëª¨ë¸")

# ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„± ìƒìœ„ 3ê°œ
top_3_features = [feature_names[i] for i in np.argsort(feature_importance)[-3:]]
print(f"   - ì£¼ìš” ì§„ë‹¨ ì§€í‘œ: {', '.join(reversed(top_3_features))}")

print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥:")
print(f"   - ì •ìƒì¸ ì •í™•í•œ ì‹ë³„: {cm[0,0]}/{cm[0,0]+cm[0,1]} ({cm[0,0]/(cm[0,0]+cm[0,1])*100:.1f}%)")
print(f"   - ì‹¬ì¥ë³‘ í™˜ì ì •í™•í•œ ì‹ë³„: {cm[1,1]}/{cm[1,0]+cm[1,1]} ({cm[1,1]/(cm[1,0]+cm[1,1])*100:.1f}%)")
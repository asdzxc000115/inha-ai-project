# -*- coding: utf-8 -*-
"""인공지능 미니프로젝트(심장병진단)_202144001_최민석

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N7-J8AHCJgNiuF9z6TOlV3FtV3pFScr3
"""

# 심장병 진단 분류 프로젝트 (202144001_최민석)
# Heart Disease Classification Project

import numpy as np
import pandas as pd
from tensorflow import keras
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# 한글 폰트 설정 (matplotlib)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# Colab에서 한글 폰트 설정 (선택사항)
# !apt-get install -y fonts-nanum
# !sudo fc-cache -fv
# !rm ~/.cache/matplotlib -rf
# plt.rc('font', family='NanumBarunGothic')

print("="*60)
print("        심장병 진단 분류 AI 모델 개발")
print("     (수업 내용 기반 - Keras & Dense Layer)")
print("="*60)

# =====================================
# 1. 데이터 로드 및 기본 정보 확인
# =====================================
print("\n1. 데이터 로드 및 탐색")
print("-"*40)

# UCI Heart Disease Dataset 로드
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
]

df = pd.read_csv(url, names=column_names)

# 기본 정보 출력
print(f"데이터 형태: {df.shape}")
print(f"특성(feature) 개수: {df.shape[1]-1}")
print(f"샘플 개수: {df.shape[0]}")

print(f"\n데이터 처음 5개 행:")
print(df.head())

print(f"\n각 특성별 정보:")
print(df.info())

# =====================================
# 2. 데이터 전처리 (수업 방식)
# =====================================
print("\n2. 데이터 전처리")
print("-"*40)

# 결측치 확인 및 처리 ('?'를 NaN으로 변환)
df_clean = df.replace('?', np.nan)

# 수치형으로 변환
for col in df_clean.columns:
    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

print("결측치 확인:")
print(df_clean.isnull().sum())

# 결측치를 평균값으로 채우기 (수업에서 배운 방법)
df_clean = df_clean.fillna(df_clean.mean())

# 타겟 변수 이진화 (0: 정상, 1: 심장병)
df_clean['target'] = (df_clean['target'] > 0).astype(int)

print(f"전처리 후 데이터 형태: {df_clean.shape}")
print(f"심장병 환자 수: {df_clean['target'].sum()}")
print(f"정상인 수: {len(df_clean) - df_clean['target'].sum()}")

# 클래스 분포 확인
print(f"\n클래스 분포:")
print(df_clean['target'].value_counts())

# =====================================
# 3. 데이터 분할 및 정규화 (수업 방식)
# =====================================
print("\n3. 데이터 분할 및 정규화")
print("-"*40)

# 특성과 타겟 분리
X = df_clean.drop('target', axis=1)
y = df_clean['target']

print(f"입력 특성: {list(X.columns)}")

# 훈련:테스트 = 8:2 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 검증 데이터 분리 (훈련 데이터의 20%)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
)

print(f"훈련 데이터: {X_train.shape}")
print(f"검증 데이터: {X_val.shape}")
print(f"테스트 데이터: {X_test.shape}")

# MinMaxScaler를 사용한 0~1 정규화 (수업에서 배운 방법)
mms = MinMaxScaler()
mms.fit(X_train)

X_train_scaled = mms.transform(X_train)
X_val_scaled = mms.transform(X_val)
X_test_scaled = mms.transform(X_test)

# DataFrame으로 변환
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

print("데이터 정규화 완료 (0~1 범위)")
print(f"정규화 후 훈련 데이터 샘플:")
print(X_train_scaled.head())

# =====================================
# 4. 모델 구성 (수업 방식 - Dense Layer)
# =====================================
print("\n4. 신경망 모델 구성")
print("-"*40)

# Sequential 모델 생성 (수업에서 배운 방식)
model = Sequential()

# Dense Layer 추가 (Fully Connected Layer)
model.add(Dense(128, activation='relu', input_shape=(13,)))  # 입력층
model.add(Dropout(0.3))                                     # 드롭아웃
model.add(Dense(64, activation='relu'))                     # 은닉층 1
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))                     # 은닉층 2
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))                     # 은닉층 3
model.add(Dense(1, activation='sigmoid'))                   # 출력층 (이진분류)

# 모델 컴파일 (수업에서 배운 설정)
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # 이진 분류용
    metrics=['accuracy']         # 수업에서 사용한 지표
)

# 모델 구조 출력
print("신경망 모델 구조:")
model.summary()

# 파라미터 수 계산 (수업 내용 참고)
total_params = model.count_params()
print(f"\n총 파라미터 수: {total_params:,}")

# =====================================
# 5. 콜백 설정 (수업에서 배운 방법)
# =====================================
print("\n5. 학습 설정")
print("-"*40)

# Early Stopping (수업에서 배운 방법)
early_stopping = EarlyStopping(
    monitor='val_accuracy',      # 검증 정확도 모니터링
    patience=15,                 # 15 에포크 동안 개선 없으면 중단
    restore_best_weights=True,   # 최고 성능 가중치 복원
    verbose=1
)

# Model Checkpoint (모델 저장)
model_checkpoint = ModelCheckpoint(
    'best_heart_disease_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

print("Early Stopping 설정 완료")
print("Model Checkpoint 설정 완료")

# =====================================
# 6. 모델 학습
# =====================================
print("\n6. 모델 학습")
print("-"*40)

# 모델 훈련
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=100,                  # 충분한 에포크
    batch_size=32,              # 수업에서 사용한 배치 크기
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)

# 최고 모델 로드
model.load_weights('best_heart_disease_model.h5')
print("\n최고 성능 모델 로드 완료")

# =====================================
# 7. 성능 평가 (수업에서 배운 지표)
# =====================================
print("\n7. 모델 성능 평가")
print("-"*40)

# 테스트 데이터로 최종 평가
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)

print(f"최종 테스트 결과:")
print(f"테스트 정확도: {test_accuracy:.4f}")
print(f"테스트 손실: {test_loss:.4f}")

# 훈련 과정 분석
best_val_accuracy = max(history.history['val_accuracy'])
best_epoch = history.history['val_accuracy'].index(best_val_accuracy) + 1

print(f"\n학습 과정 분석:")
print(f"최고 검증 정확도: {best_val_accuracy:.4f} (에포크 {best_epoch})")
print(f"총 학습 에포크: {len(history.history['loss'])}")

# 예측 결과 샘플 확인
y_pred_proba = model.predict(X_test_scaled)
y_pred = (y_pred_proba > 0.5).astype(int)

print(f"\n예측 결과 샘플 (처음 10개):")
for i in range(10):
    actual = y_test.iloc[i]
    predicted = y_pred[i][0]
    confidence = y_pred_proba[i][0]
    status = "✓" if actual == predicted else "✗"
    print(f"{status} 실제: {actual}, 예측: {predicted}, 확신도: {confidence:.3f}")

# =====================================
# 8. 시각화 (학습 과정)
# =====================================
print("\n8. 학습 과정 시각화")
print("-"*40)

# 학습 곡선 그리기
plt.figure(figsize=(15, 5))

# 정확도 곡선
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# 손실 곡선
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# 특성 중요도 (가중치 크기 기반)
plt.subplot(1, 3, 3)
weights = model.layers[0].get_weights()[0]  # 첫 번째 Dense layer 가중치
feature_importance = np.mean(np.abs(weights), axis=1)
feature_names = X.columns

# 상위 10개 특성만 표시
top_indices = np.argsort(feature_importance)[-10:]
plt.barh(range(len(top_indices)), feature_importance[top_indices])
plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])
plt.title('Feature Importance (Top 10)')
plt.xlabel('Importance')

plt.tight_layout()
plt.show()

# =====================================
# 9. 혼동행렬 및 상세 분석
# =====================================
from sklearn.metrics import confusion_matrix, classification_report

print("\n9. 상세 성능 분석")
print("-"*40)

# 혼동행렬
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Normal', 'Heart Disease'], yticklabels=['Normal', 'Heart Disease'])
plt.title('Confusion Matrix')
plt.ylabel('True')
plt.xlabel('Predicted')
plt.show()

# 분류 리포트
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Heart Disease']))

# =====================================
# 10. 결론 및 인사이트
# =====================================
print("\n" + "="*60)
print("                  프로젝트 결론")
print("="*60)

print(f"✅ 최종 테스트 정확도: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")
print(f"✅ 사용된 기법:")
print(f"   - Dense Layer (Fully Connected Layer)")
print(f"   - MinMaxScaler를 이용한 정규화")
print(f"   - Early Stopping으로 과적합 방지")
print(f"   - Model Checkpoint로 최고 모델 저장")
print(f"   - Dropout으로 일반화 성능 향상")

print(f"\n🏥 의료진을 위한 인사이트:")
print(f"   - 심장병 진단 보조 도구로 활용 가능")
print(f"   - {test_accuracy*100:.1f}% 정확도로 신뢰할 만한 성능")
print(f"   - 총 {total_params:,}개 파라미터로 효율적인 모델")

# 가장 중요한 특성 상위 3개
top_3_features = [feature_names[i] for i in np.argsort(feature_importance)[-3:]]
print(f"   - 주요 진단 지표: {', '.join(reversed(top_3_features))}")

print(f"\n📊 모델 성능:")
print(f"   - 정상인 정확한 식별: {cm[0,0]}/{cm[0,0]+cm[0,1]} ({cm[0,0]/(cm[0,0]+cm[0,1])*100:.1f}%)")
print(f"   - 심장병 환자 정확한 식별: {cm[1,1]}/{cm[1,0]+cm[1,1]} ({cm[1,1]/(cm[1,0]+cm[1,1])*100:.1f}%)")